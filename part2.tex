\chapter{Практическая часть. Исследование образа губернатора омской области в местных Интернет-СМИ} \label{chapt2}
\section{Определение целей исследования} \label{sect2_1}
В данной части работы мы разработаем и проведём исследование, на примере которого будут показаны возможности метода интеллектуального анализа текста в социологии. Цель исследования на данном этапе состоит в том, чтобы с помощью интеллектуального анализа текста выявить некоторые характеристики дискурса о мэре г. Омска в местных Интернет-СМИ. Такими характеристиками являются:
\begin{enumerate}
\item Распределение статей с упоминанием мэра во времени. Нас будет интересовать, в какие месяцы или дни недели активизируется соответствующий дискурс. В контексте этой задачи мы сравним распределение статей во времени из генеральной совокупности и распределение статей из выборочной совокупности с тем, чтобы определить, значимо ли они различаются. В зависимости от полученных результатов можно выдвинуть гипотезы, объясняющие полученное распределение.
\item Количество комментариев. Для определения заинтересованности читателей в данной теме, сравним количество комментариев к статьям о мэре со средним количеством комментариев.
\item Тема документа, в котором упомянут мэр. С помощью алгоритмов тематического моделирования мы определим тематический контекст статей из выборки, изучим его распределение во времени и сравним его темами в генеральной совокупности на предмет наличия общих и особенных тем. Попутно мы оценим эффективность работы алгоритма тематического моделирования. % надо сравнить темы, которые алогоритм присвоил статьям о мэре, когда они были частью генеральной совокупнстьи. если алгоритм выделил эти статьи в собоую тему, то всё норм. А ещё надо посмотреть, в какием темы раскидало статьи о мэре, при выделение тем в генеральной совокупности. 
\item Попытка анализа настроений. Нам будет произведено сравнение эмоций по выборке, где упомянут мэр и остальной, а так же в различных темах.
% полноценный анализ тональности не будет использован так как нет готовых инструментов и он плохо работает на больших документах.
% Возможные варианты: 1) Выделить наиболее употребительные слова как тут: http://globalvoicesonline.org/2014/03/02/sentiment-analysis-of-russian-tweets-about-war-in-crimea/
% 2) Выделить темы, где эмоции проявляются наиболее сильно. Таким образом можно определить самые важные темы.
% 3) Тональный анализ можно произвести на названиях статей. Для анализа использовать sentimental или sentistrength
\end{enumerate}
\section{Оценка доступности и характера данных. Сбор данных.} \label{sect2_2}
Прежде чем начать сбор данных, нам придётся поставить перед собой несколько вопросов, представляющих особенную трудность в данного типа исследованиях. А именно, необходимо определить, что будет являться носителем знаний по исследуемой проблеме (т. е. эмпирическим объектом исследования), каковы границы генеральной совокупности, какой метод будет являться адекватным для построения выборочной совокупности, как определить качественные и количественные характеристики выборки, каковы критерии репрезентативности выборки \cite{methodlogy_internet}.

\textbf{Определение эмпирического объекта.} В самом общем виде можно сказать, что источником знаний о проблемах, затронутых в данном исследовании является статьи в Интернет-СМИ Омской области. Углубляясь дальше, мы должны решить какие аспекты статьи нас интересуют. Статья в Интернет-СМИ -- не просто один лишь неструктурированный текст. Это документ, который имеет свою структуру. В этой структуре нас буду интересовать такие элементы как собственно текст, название, дата публикации, количество комментариев, сами комментарии, принадлежность к тому или иному СМИ. Первая причина, по которой они были выбраны состоит в представленности этих элементов в статьях каждого из рассматриваемых нами Интернет-СМИ. Количество просмотров и ключевые слова (тэги), например, на некоторых ресурсах бывают не указаны. Другая причина -- достаточность данных элементов для решения исследовательских задач.

\textbf{Определение генеральной совокупности.} Генеральную совокупность в данном исследовании составляют статьи Интернет-СМИ г. Омска. Интернет-СМИ — веб-сайт, ставящий своей задачей выполнять функцию средства массовой информации (СМИ) в сети Интернет. По данным Агентства Региональных Исследований за июнь 2014 года в Омске работает около 18 Интернет-СМИ с месячным количеством уникальных посетителе в месяц более 10000\cite{ari_rating}. 

\textbf{Определение выборочной совокупности.} Использование данных со всех возможных ресурсов -- очень трудоёмкая задача, поскольку требует практически полного переписывания соответствующей части программы, ответственной за собственно сбор данных, и частичной переработки модуля предварительной обработки. Вполне привычным для социолога решением будет конструирование выборки. Однако как рассчитать выборку, если не известны объёмы генеральной совокупности. А даже если мы знали количество статей каждого из рассматриваемых Интернет-СМИ за любой промежуток времени, разве было бы корректно использовать традиционные методы определения выборочной совокупности в такого типа исследованиях? Эта аналогия видится некорректной по причине кардинального различия эмпирических объектов -- человека и текста. При определении людей в качестве эмпирических объектов исследования социолог как правило предполагает, что они в равной степени могут служить источником информации о проблеме. Исключение из этого правила встречается, когда исследователь отдельно изучает мнение экспертов. Но экспертные опросы -- это отдельная часть исследования, в которой как правило используются другие методы сбора и анализа информации.

В нашем случае, исходя из цели исследования -- определение образа мэра, транслируемого Интернет-СМИ в обществе, -- важна не сама статья, а влияние, оказываемое ей на общество. Именно это влияние и определяет степень, с которой статья может служить источником информации о проблеме. Его прямым индикатором служит количество просмотров данной статьи. Но не все Интернет-СМИ предоставляют эту информацию, поэтому можно опереться на косвенный индикатор -- количество просмотров всех статей исследуемого ресурса. То есть мы предполагаем, что чем больше совокупное количество просмотров у одного ресурса, тем более сильно влияние каждой его отдельной статьи. Для верности этого тезиса необходимо только чтобы общее количество статей в каждом из ресурсов не сильно отличалось друг от друга. {\underline{привести данные}}

Приведённые выше рассуждения позволяют считать, что при выборе Интернет-СМИ, статьи которых будут подвергнуты анализу, стоит опираться на общее количество просмотров. Однако это не даёт ответа на вопрос, сколько и какие статьи должны быть отобраны. Способов расчёта этих значений на сегодняшний день нет. Существуют публикации отдельных исследователей из разных отраслей, разрабатывающих свой методологический аспект при исследований текстов в сети Интернет. Определение выборочной совокупности прежде всего зависит от того, что информация о чём важно для исследования. На основании этого определяет эмпирический объект (это может быть текст, комментарий, отдельное высказывание и др.) и принцип определения его важности (например, степень влияние на общество). В любом случае сохраняет один принцип -- из выборки необходимо получить репрезентативное подмножество, -- но исследователи пытаются достичь его разными путями.

З. Папачарисси в при исследовании блогосферы определяла в качестве генеральной совокупности все блоги, расположенные на платформе blogger.com. Она объясняет свой выбор тем, что это наиболее популярный и большой по числу блоггеров англоязычный сайт, который предоставляет возможности для персональных публикаций в стиле любительской журналистики. Любой блог, по мнению Папачарисси, размещённый это этом сайте, представлял собой единицу анализа, отвечающую по своим характеристикам признакам принадлежности к генеральной совокупности. Однако нельзя согласиться, что блоги с blogger.com репрезентативны относительно всей блогосферы. Выборочную совокупность блогов Папачарисии составляла использую случайную отправную точку и случайный выборочный интервал. Однако исследователем не было оговорено, какием именно данные вводились в поисковую систему для поиска релевантных блогов и какие именно блоги считались релевантными, сколько блогов входило в генеральную совокупность и почему было отобрано именно 260. К тому же использование поисковых систем для поиска блогов выглядит сомнительно: алгоритмы данных систем неизвестны исследователю и нельзя сказать, почему были отобраны эти блоги, а не иные.

Этот пример исследования был приведёт нами, чтобы проиллюстрировать отсутствие единой позиции в способах определения выборочной и генеральной совокупности в Интернет-исследованиях. Каждый исследователь придумывает сам, каким способом наиболее полно реализовать принципы выборки.

В нашем случае необходимо определить несколько Интернет-СМИ, все статьи которых будут отобраны для исследования. Мы выяснили, что при определении значимости, веса статьи определяющей характеристикой является количество просмотров. Хотя Интернет-СМИ в Омске и немало, не все из них одинаково крупны. Судя по тем же данным АРИ, в Омск существует всего четыре новостных ресурса, страницы которых просматривают более одного миллиона раз в месяц. В процентном отношении они занимают 65\% рынка омских Интернет-СМИ. Представляется, что анализ статей, получивших более половины всех просмотров является достаточным основанием для выделения их в качестве выборочной совокупности, по результатом анализа которой можно будет делать выводы об омских Интернет-СМИ в целом. Таким образом в исследовании будут проанализированы все новостные статьи с сайтов \href{http://gorod55.ru/}{gorod55.ru}, \href{http://bk55.ru/}{bk55.ru}, \href{http://ngs55.ru/}{ngs55.ru}, \href{http://omskinform.ru/}{omskinform.ru} за период с 1 сентября 2013 по 1 сентября 2014. Новостными статьями будут считаться те, которые публикуются на данном ресурсе в разделе <<Новости>>. Статьи из категорий <<Работа>>, <<Объявления>>, <<Блоги>> и др. в анализе не участвуют.

Определившись с данными, которые необходимо собрать, нужно решить, каким способом это сделать, т. е. с использованием каких инструментов и технологий будет производится сбор данных. Для этого мы будем использовать язык программирования Python. Основанием для такого выбора является его простота, поддержка многопоточности, что полезно для более быстрого сбора данных, наличие сторонних библиотек, что позволяет избежать написание рутинного кода, а также то, что обработка и анализ данных также будет производится на этом языке -- это обеспечивает некоторую консистентность исследования. Ближайшей альтернативой данному решению видится использование программной платфоры node.js из-за хорошей поддержки асинхронных запросов (и, следовательно, высокой скорости) и наличия множества качественных библиотек для сбора данных или языка R, который традиционно популярен в академической среде для сбора и анализа данных.

Для хранения данных будет использована база данных MongoDB. Как говорилось выше, в БД будут присутствовать следующие поля: называние статьи (title), содержимое статьи (content), ссылка на статью (url), дата публикации (date), количество комментариев (commentsCount) и список комментариев к статье (comments). Статьи с каждого источника будут храниться в отдельной коллекции.

Результаты сбора данных следующие:
\begin{itemize}
\item C сайта gorod55.ru было собрано 6302 статьи
\item Больше всего новостных статей за указанный промежуток времени было опубликовано на bk55.ru -- 14078 статей  на bk55.ru
\item Наименьшее количество статей -- 4780 -- было найдено на ngs55.ru
\item 8727 статей по указанным параметрам было собрано с сайта omskinform.ru
\end{itemize}

Всего таким образом в анализе участвовало 33887 статей.

На этом этапе крайне важно контролировать корректность и полноту собираемых данных. Сложнее всего было с сайтом bk55.ru, поскольку в нём использовались несколько различных шаблонов для отображения информации, каждый из которых необходимо было отследить и создать под него набор правил для извлечения данных.

\section{Предварительная обработка данных} \label{sect2_3}

Предварительная обработка данных -- один из важнейших этапов в анализе текста. Наша цель на этом этапе -- удаление несущественных и вносящих помехи данных и преобразование данных к удобному для анализа виду.

На самом деле удалять несущественные данные мы начали ещё на этапе сбора данных, поскольку перед записью в базу данных весь текст, если это было необходимо, очищался от html-разметки. Преобразование же данных на том этапе заключалось в конвертации текста, содержащего информацию о дате публикации, в специальный тип данных, позволяющий обращаться к этим данным как к дате, например, производить выборку статей за определённый период.

Следующий шаг в предварительной обработке данных заключается в удалении из каждой статьи признаков, свидетельствующих о её принадлежности к какому-либо источнику. Если посмотреть на полученные тексты, то можно увидеть, что редакция каждого СМИ устанавливает собственные правила оформления документов, касающиеся оформления ссылок на источники данных, фотографий, указание имён авторов. В случаем если эти отличительные черты не будут устранены, алгоритмы тематического моделирования, которые мы в дальнейшем собираемся применить к собранному корпусу текстов, будут стремиться образовать темы вокруг источников. Процедура унификации статей из различный источников достаточно трудоёмка и требует ручного анализа множества статей с каждого из них, с тем чтобы выявить в них специфические черты для каждого сайта. Такими чертам могут быть имена журналистов данного издания или правила оформления фото и видео материалов (например, около каждой фотографии может указываться копирайт).

Например, чтобы удалить имена журналистов из текстов статей на сайте bk55.ru, необходимо было во-первых, составить их список. Для составления списка, была написана небольшая программа, выводящая два последних слова каждого документа, если они начинались с заглавной буквы (как правило имена авторов указывались в конце документа, хоть и не всегда). Из полученного списка примерно в пятьсот пар были вручную отсеяны пары, не являющиеся именем и фамилией. Те пары из этого списка, которые встречались больше двух раз, считались нами именем и фамилией журналистов сайта bk55.ru. На последнем этапе фамилии журналистов удалялись из каждого документа. К тому же, так как после имён журналистов часто указывалась другая мета-информация (главным образом ссылки источники информации), то также удалялся весь текст после имён, если по размеру этот текст не превышал определённое количество символов (чтобы предотвратить удаление не мета-информации). 

После устранения специфической информации данные из различных источников объединялись в единый корпус и подвергались дальнейшей обработке. Обработка заключалось в следующем:
\begin{enumerate}
\item Перевод текста в нижний регистр
\item Токенизация % неразрывные пробелы
\item Удаление пунктуации
\item Стемминг
\item Удаление стоп-слов
\item Замена слов
\end{enumerate}

Что касается перевода текста в нижний регистр и удаления пунктуации, то это достаточно тривиальные процедуры, не требующие особых объяснений.

Другим этапов предварительной обработки текста является токенизация. Именно с неё начинается обработка естественного языка как наука и как конкретная деятельность \cite{Webster1992}. Под токенизацией понимают процесс сегментации текста на отдельные части, называемые токенами. Именно токены являются теми первичными элементами, которые непосредственно участвуют в процессе анализа. 

Выделяют два основных признака токена -- лингвистическая значимость и методологическая полезность \cite[стр. 1106]{Webster1992}. В языках с иероглифической письменностью токенизация является серьёзной проблемой, поскольку один иероглиф может обозначать как морфемы (в таком случае он не удовлетворяет требованиям для того, чтобы считаться токеном), так и целые слова. В английском и русском языках проблема токенизации не стоит так остро и чаще всего токены определяются через пробелы между словами и знаки препинания. Тем не менее, даже в этих языках существуют определённые нюансы.

Нами было протестировано несколько алгоритмов токенизации (токенайзеры TreebankWordTokenizer, WordPunctTokenizer, PunctWordTokenizer и WhitespaceTokenizer из программы NLTK и токенайзер из Pattern). Корректнее всех выделял токены токенайзер из программы \href{http://www.clips.ua.ac.be/pattern}{Pattern}. Например, он единственный интерпретировал url'ы как цельные токены, не выделяя в них отдельные сегменты, на основе знаков препинания.

После токенизации и удаления токенов, являющихся знаками препинания, мы перешли от представления документов как набора символов к документам как списку слов. Дальнейшие наши шаги будут направлены на уменьшение длинны этого списка, т. е. на снижение как общего количества токенов, так и количества их уникальных единиц. Необходимость этих шагов обусловлена желанием снизить вычислительную сложность при анализе данных.

Первый шаг направлен на снижение количества уникальных токенов. Для компьютера различные формы одного и того же слова являются совершенно разными словами. Существует два способа для приведения словоформ к одной лексеме. Первый, самый простой, называется стемминг. Он состоит в отсечении слово- и формообразующих частей -- префиксов, суффиксов, окончаний, в результате чего остаётся основа слова -- неизменная часть, выражающая его лексическое значение.

Более сложным подходом к решению проблемы унификации словоформ является лемматизация. Лемматизация -- это процесс приведения словоформы к лемме — её нормальной (словарной) форме. В русском языке нормальная форма имени существительного имеет именительный падеж и единственно число, для прилагательных добавляется требование мужского рода, а глаголы, деепричастия и причастия в нормальной форме должны стоят в инфинитиве.

Для постановки слова в нормальную форму необходимо иметь словарь, где для каждого слова определены его характеристики, т. е. часть речи, падеж, число, род, форма глагола (если это глагол). Создание такого словаря требует колоссальных трудов. В отличии от этого, стемминг предполагает наличие лишь списка приставок, суффиксов и окончаний, количество которых исчисляется несколькими десятками. К счастью, для русского языка существует так необходимый для лемматизации словарь, созданный в рамках проекта \href{http://opencorpora.org/}{OpenCorpora}. Использующая этот словарь программа \href{https://pymorphy2.readthedocs.org/}{pymorphy2} позволяет приводить слова к нормальной форме.

Между вышеозначенными способами мы выбрали лемматизацию, поскольку получаемые в результате этого процесса леммы легче интерпретировать, чем усечённые основы слов, значение которых не всегда легко восстановить.

Дальнейшие усилия по уменьшению количества токенов связаны с удалением так называемых стоп-слов. Эти слова сами по себе почти не неся полезного смысла, тем не менее, необходимы для нормального восприятия текста. Чаще всего к разряду стоп-слов относятся служебные части речи -- предлоги, союзы, частицы. Будучи широко распространёнными в тексте, они мало могут сказать о его теме.

В качестве базы для списка стоп-слов был использован список русских стоп-слов из программы NLTK. Однако его нельзя считать достаточно полным. Включая в себя 151 слово данный список покрывает лишь самые основные случаи. Для его пополнения необходимо обратиться к собранным ранее данным. На их основе был составлен список наиболее часто встречающихся в корпусе токенов. Среди них были выбраны несколько десятков слов, наиболее точно подходящие под описание стоп-слов (это, который, такой, некоторый, другой, тот и др.), которые затем были добавлены в соответствующий список. Представляется, что такой список, дополненный словами, выбранными из числа наиболее распространённых, является достаточно полным, поскольку стоп-слова по своему характеру всегда относятся к наиболее часто встречающимся в тексте. Редкие слова как правило свидетельствуют о принадлежности текста к какой-либо теме, а потому не могут относиться к разряду стоп-слов.

Как видно, в общих чертах данный набор процедур повторяет составляющие предварительной обработки данных из методологии CRISP-DM.

% об этом в конце
Также желательно после каждой операции с данными на этапе предварительной обработки контролировать последствия производимых изменений. Так как большинство операций сводится к изменению количества определённых слов, следует, просматривать список самых распространённых слов в корпусе.

%Для успешного выполнения поставленной задачи необходимо вручную проанализировать большое количество статей с каждого источника с тем, чтобы выявить в них специфические образцы оформления. На сайте bk55.ru (здесь, как и на этапе сбора данных, ситуация обстояла сложнее всего), например, такими специфическими образцами были:
%Стоит ли описывать?

\section{Анализ данных}

% Прям по этапам, выделенным в теоретической части, разобрать, что делать


\clearpage