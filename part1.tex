\chapter{Интеллектуальный анализ данных как метод анализа данных} \label{chapt1}

\section{Место в структуре исследовательских методов} \label{sect1_1}

\subsection{Дуальность статистики}

\epigraph{Если данные говорят с вами, значит вы --- байесовец.}{Филип А. Шродт \cite[стр. 11]{Schrodt2010}}

\epigraph{Формально, теорема Байеса -- это просто математическая формула. Однако её значение гораздо глубже. Теорема Байеса подводит нас к тому, что необходимо иначе взглянуть на процесс выдвижения и проверки идей.}{Нэт Сильвер\footnotemark\cite{silver2012}}

%Bayes’s theorem is nominally a mathematical formula. But it is really much more than that. It implies that we must think differently about our ideas—and how to test them.

\footnotetext{Американский статистик, давший самые точные прогнозы президентских выборов в США в 2008 и 2012 гг.}

Для определения того места, которое занимают методы text mining, следует сказать о двух основных направлениях, в которых развивалась математическая статистика и понимание понятия вероятности. Как и многое другое, дуальность статистики берёт своё начало из философского спора Аристотеля и Платона \cite[стр. 7]{handbook_stat_dm}. Аристотель считал, что реальность может быть познана только эмпирически и что исследователь должен тщательно изучать вещественный мир вокруг себя. Он пришёл к убеждению, что можно разложить сложную систему на элементы, детально описать эти элементы, соединить их вместе и, затем, понять целое. Именно таким механистичным путём долгое время следовала наука. Однако в дальнейшем стало понятно, что не всегда целое можно представить как простую сумму частей, его составляющих. Часто, будучи соединёнными вместе, совокупность этих частей приобретает новое качество.

В отличии от своего ученика, Платон считал, что свойством подлинного бытия обладают только идеи, а человек может лишь воспринимать и воплощать в вещах их смутные очертания. Для Платона идея (целое) была большим, чем сумма её материальных проявлений.

Эта дихотомия восприятия реальности проявляется во многих аспектах человеческой мысли, в том числе и в сфере статистического знания, в котором с XVIII в. существует две основных философских позиции относительно того, как применять вероятностные модели. Первая определяет вероятность как нечто, заданное внешним миром. Вторая утверждает, что вероятность существует в головах людей.\cite[стр. 18]{Christensen2010}. В русле первого подхода возникли вначале классическая и затем развивающая её частотная концепции вероятности. Второй подход нашёл выражение в концепции байесовской вероятности.

Сторонники классического подхода исходят из того, что истинные параметры модели не случайны, а аппроксимирующие их оценки случайны, поскольку они являются функциями наблюдений, содержащих случайный элемент. \cite[стр. 5-6]{Zellner1980} Параметры модели считаются не случайными из-за того, что классическое определение вероятности исходит из предположения равновозможности как объективного свойства изучаемых явлений, основанного на их реальной симметрии \cite[стр. 24]{Gnedenko2005}. На такое представление о вероятности повлияло то, что в начале своего развития теория вероятности применялась прежде всего для анализа азартных игр. Суждение вида <<вероятность выпадения шестёрки при бросании игрального кубика равняется 1/6>> основывается на том, что любая из шести граней при подбрасывании на удачу не имеет реальных преимуществ перед другими, и это не подлежит формальному определению. Таким образом, вероятностью случайного события $A$ в её классическом понимании будет называться отношение числа несовместимых (не могущих произойти одновременно) и равновозможных элементарных событий $m$ к числу всех возможных элементарных событий $n$:
\begin{eqnarray}
P(A)=\frac{m}{n}
\end{eqnarray}

Однако такое определение наталкивается на некоторые непреодолимые препятствия, связанные с тем, что не все явления подчиняются принципу симметрии. Например, из соображений симметрии невозможно определить вероятность наступления дождливой погоды. Для преодоления подобных трудностей был предложен статистический или частотный способ приближенной оценки неизвестной вероятности случайного события, основанный на длительном наблюдении над проявлением или не проявлением события $A$ при большом числе независимых испытаний и поиске устойчивых закономерностей числа проявлений этого события. Если в результате достаточно многочисленных наблюдений замечено, что частота события $A$ колеблется около некоторой постоянной, то мы скажем, что это событие имеет вероятность. Данный тип вероятности был выражен Р. Мизесом в следующей математической формуле:
\begin{eqnarray}
p=\lim_{x\to\infty}\frac{\mu}{n},
\end{eqnarray}
где $\mu$ --- количество успешных испытаний, $n$ --- количество всех испытаний \cite[стр. 46-47]{Gnedenko2005}. Вероятность здесь понимается как частота успешных исходов я является чисто объективной мерой, поскольку зависит лишь от точного подсчёта отношения количества успешных и неуспешных событий. 

Основываясь на этом подходе, статистика занималась созданием вероятностных моделей, которые включали в себя параметры, которые, как предполагалось, связаны с характеристиками исследуемой выборки. Параметры никогда не могут быть известны с абсолютной точностью до тех пор, пока мы не исследуем всю генеральную совокупность \cite[стр. 1]{Christensen2010}. До тех пор всегда существует вероятность отклонить гипотезу, когда она на самом деле верна, т. е. совершить ошибку первого рода. Для обозначения вероятности такой ошибки частотники используют понятие уровня значимости $\alpha$. Именно вероятность ошибки первого рода частотники ставят во главу анализа, определяя вероятность события. После каждого своего утверждения они обычно добавляют <<... на доверительном уровне в 95\%>>, подразумевая, что исследователь допускает вероятность ошибки в пяти процентах случаев (при $\alpha = 0,05$) \cite[стр. 10-11]{handbook_stat_dm}.

Иногда параметры вообще невозможно интерпретировать применительно к реальной жизни, поскольку модели редко бывают абсолютно верными. Модели, как мы надеемся, --- это некоторые полезные приближения к истине, на основании которых можно делать прогнозы. Тем не менее прежде всего классическое статистическое исследование сосредоточено на оценке параметров, а не на предсказании \cite[стр. 1]{Christensen2010}.

%Развивая статистическую модель данных, байесовский подход предоставляет дополнительные вероятностные модели для всех неизвестных параметров в модели данных. Этот подход заключается в моделировании неуверенности в параметрах с помощью научной информации эксперта. Эта информация называется априорной. Экспертная информация должна быть получена независимо от анализируемых данных.
% Цель исследования: оценка параметров --- предсказание

Частотный подход доминировал в XX веке, придя на смену другому пониманию вероятности, связанном с именем английского математика Томаса Байеса\cite[стр. 2]{Efron2005}. Сущность байесовского подхода составляют три элемента: априорная вероятность, исходные статистические данных данные, постаприорная вероятность.

Байесовская статистика начинает построение своей модели при помощи понятия априорной вероятности, с помощью которой описывается текущее состояние наших знаний, относительно параметров распределения \cite[стр. 18]{Christensen2010}. Априорная вероятность, такие образом, --- это степень нашей уверенности в том, что исследуемый параметр примет то или иное значение ещё до начала сбора исходных статистических данных. На этом основании байесовское понимание вероятности относят к группе субъективистских трактовок вероятности. Чаще всего предполагается, что для оценки степени уверенности необходимо привлечь экспертов, чьё субъективное свидетельство позволит избежать действительной многократной реализации интересующего нас эксперимента\footnote{Не следует путать субъективный характер байесовской вероятности в целом с внутренним разделением сторонников данного подхода на объективистов и субъективистов, основанном на различном отношении к роли рациональных ограничений при определении априорной вероятности. В качестве примера различного подхода к определению априорной вероятности рассмотрим ситуацию, где событием является изъятие мячика из урны, наполненной красными и чёрными мячиками --- и это всё, что нам известно об урне. Зададим вопрос: какова априорная вероятность (до изъятия мячика), что изъятый мячик будет чёрного цвета? Субъективисты, считающие роль рациональных ограничений относительно небольшой, ответят, что любая вероятность от 0 до 1 может быть рациональной, так как по их мнению наша оценка априорной вероятности зависит большей частью от нерациональных факторов --- социализации, свободного выбора и др. Объективисты же будут настаивать, что априорная вероятность в данном случае равняется 1/2, поскольку именно такая вероятность в соответствии с принципом неопределённости Джейнса инвариантна к к размерам и трансформациям мячиков \cite{Talbott2013}.} \cite[стр. 34]{Aivazyan2001}.

Следующий элемент --- это исходные статистические данные. По мере их поступления статистик пересчитывает распределение вероятностей анализируемого параметра, переходя от априорного распределения к апостериорному, используя для этого формулу Байеса:
\begin{eqnarray}
P(A|B)=\frac{P(B|A)P(A)}{P(B)},
\end{eqnarray} 
где $P(A)$ --- априорная вероятность гипотезы $A$, $P(A|B)$ --- вероятность гипотезы $A$ при наступлении события $B$ (апостериорная вероятность), $P(B|A)$ --- вероятность наступления события $B$ при истинности гипотезы $A$, $P(B)$ --- полная вероятность наступления события $B$. Суть формулы в том, что она позволяет переставить причину и следствие: по известному факту события вычислить вероятность того, что оно было вызвано данной причиной. Эту формулу также называют формулой обратной вероятности. Процесс пересмотра вероятностей, связанных с высказываниями, по мере поступления новой информации составляет существо обучения на опыте\footnote{Понятие <<обучение на опыте>> ещё встретится в данной работе, поскольку именно оно составляет суть машинного обучения --- подраздела науки искусственного интеллекта, методы которого используются в text-mining.} \cite[стр. 21-22]{Zellner1980} и  является одним из возможных способов формализации и операционализации следующего тезиса: <<\textit{степень нашей разумной уверенности в некотором утверждении (касающемся, например, неизвестного численного значения интересующего нас параметра) возрастает и корректируется по мере пополнения имеющейся у нас информации относительно исследуемого явления}>> \cite[стр. 93]{Aivazyan2008}. В частотном подходе данный тезис интерпретируется в свойстве состоятельности оценки неизвестного параметра: чем больше объём выборки, на основании которой мы строим свою оценку, тем большей информацией об этом параметре мы располагаем и тем ближе к истине наше заключение. Специфика байесовского подхода к интерпретации этого тезиса основана на том, что вероятность, понимаемая как количественное значение степени разумной уверенности в справедливости некоторого утверждения, пересматривается по мере изменения информации, касающейся этого утверждения. Поэтому в данном подходе вероятность всегда есть условная вероятность, при условии нынешнего состояния информации (в русле классического подхода исследователь скорее склонен рассматривать совместную вероятность\cite[стр. 5]{handbook_stat_dm}).

Дискуссии вокруг того, какой же метод предпочтительней, ведутся уже не одно столетие, породив великое множество книг и статей на эту тему \cite{Efron2005}, \cite{Jeffreys1983}, но к однозначному выводу прийти не удалось. Острота дискуссии объясняется тем, что спор сторонников байесовского и частотного подхода к статистическому выводу отражает два различных взгляда на способ добычи научного знания. Именно поэтому от ответа на этот, казалось бы, локальный вопрос математической статистики зависит развитие всей науки.

Так или иначе, в 1980-х годах, стало ясно, что частотный подход к статистическому выводу не достаточно хорошо подходит для анализа нелинейных отношений в больших объёмах данных, производимых сложными системами при моделировании процессов реального мира\cite[стр. 10]{handbook_stat_dm}. Для преодоления этих ограничений частотники создали нелинейные версии параметрических методов, такие как множественный нелинейный регрессионный анализ.

В то время как в частотном подходе происходили изменения, немногочисленные сторонники байесовского подхода упрямо продвигали свою точку зрения на модель статистического вывода. Как оказалось, байесовская модель лучше подходит для поиска ответов на некоторые практические вопросы, поскольку полнее учитывает прошлую информацию и располагает к предсказаниям. Например, намного важнее минимизировать вероятность ложноотрицательного диагностирования некоторой опухоли как раковой, чем вероятность её ложноположительного определения (ошибка первого рода). 

Продемонстрируем на примере различия в работе частотных и байесовских методов проверки гипотез. Предположим, некоторый стрелок утверждает, что точность его стрельбы составляет 75\%. Когда стрелка попросили продемонстрировать свои навыки, он попал в мишень только 2 раза из 8. Какова вероятность, что стрелок сказал правду о своих навыках.

\textbf{Решение задачи в частотном подходе}. Гипотеза $H_{0}$ --- стрелок сказал правду. Испытание --- стрельба по мишени. Событие $A$ --- попадание в мишень. $P(A)$ постоянная и равна 0,75. Для расчёта вероятности того, что событие $A$ наступило не более 2 раз в 8 независимых испытаниях, применим формулу Бернулли для количества успешных испытаний $k$ = 0, 1, 2 и получим, что $P(A\leq 2)=0,0042$. Следовательно, при уровне значимости $\alpha$ = 0,05 следует признать невероятным, что точность стрелка составляет 75\%, гипотеза $H_{0}$ отвергается. 

Отметим некоторые особенности данного решения. Во-первых, для решения задачи мы фактически использовали только умение рассчитывать совместную вероятность, ведь формула Бернулли является сокращённым видом расчёта совместной вероятности успешных комбинаций. Во-вторых, мы решили, что если гипотеза верна, то вероятность отклонить гипотезу, когда она на самом деле верна должна быть не менее 5\%, т. е. нам важно, чтобы вероятность ложноположительного ответа была ниже определённой границы. Вероятность ложноотрацательного ответа не рассматривается.

\textbf{Решение задачи в байесовском подходе}. В данном подходе мы не проверяем гипотезу, а рассчитываем условную вероятность события $A$ (точность стрелка составляет 75\%) при условии события $B$ (стрелок попал в мишень не более 2 раз из 8). Прежде всего нам нужно оценить априорную вероятность события $A$. Это можно сделать, посмотрев статистику стрельбы остальных стрелков. Предположим, мы выяснили, что 70\% стрелков имеют точность в 75\%. Следовательно, $P(A)=0,7$. $P(B|A)$ мы уже рассчитали в частотном подходе. $P(B)$ легко рассчитывается по формуле полной вероятности. По формуле Байеса $P(A|B)=0,0301$. 

Как видно из этого примера, в байесовском подходе другая логика расчёт вероятности: на основании данных рассчитывается вероятность того, что $H_{0}$ верна, в то время как раньше мы рассчитывали вероятность того, что стрелок поразил мишень не более 2 раз в 8 независимых испытаниях. Данные, полученные с помощью данного метода, данные можно использовать более продуктивно. Предположим, что мы рассчитываем не вероятность того, что стрелок с определённым умениями поразил мишень какое-то количество раз, а вероятность наличия тяжёлого заболевания у человека с каким-то количеством положительных тестов. В случае частотного подхода мы узнаем, какова вероятность того, что больной человек получит н-ое количество положительных тестов. Байесовский же подход позволяет узнать именно то, что нам надо --- вероятность того, что человек, получивший н-ое количество положительных тестов, болен. Другой плюс данных методов --- они работают даже если размер выборки равен нулю. В таком случае байесовская вероятность равна априорной.

Проведение тестирования на статистическую значимость оценивает лишь вероятность получения похожего результата с другим набором данных при сохранении тех же самых условий. Однако оно предоставляет ограниченную картину такой вероятности, поскольку в расчёт принимается ограниченное количество информации относительно исследуемых данных. И оно само по себе не способно вам сказать, являются ли основные положения исследования верными и будут ли подтверждены полученные результаты в различных условиях \cite{bastian2013}. Уровень p говорит только о вероятности получения результата при (обычно) совершенно нереалистичных условиях нулевой гипотезы. А это совсем не то, что мы хотим узнать, --- обычно мы хотим знать величину эффекта независимой переменной с учётом имеющихся данных. Это байесовский вопрос, а не частотный. Вместо этого значение p часто интерпретируется так, будто бы оно показывало силу ассоциации \cite[стр. 11]{Schrodt2010}.

С другой стороны, и у байесовского метода имеются несколько недостатков. Одним из них является необходимость привлекать для расчёта априорные данные, которые могут быть недоступны. А если они и доступны, то, как отмечалось выше, часто носят субъективный характер. Другой недостаток --- сложность вычислений. В вышеописанном примере для вычисления байесовской вероятности нам необходимо было вычислить частотную вероятность, полную вероятность, и, наконец, собственно байесовскую вероятность. Сложность байесовских вычислений частично объясняет тот факт, что байесовские методы вновь обрели популярность с развитием вычислительной техники. Следующий недостаток байесовского метода --- неинтуитивность, непонятность его результатов для обыденного сознания. Именно на этой неинтуитивности построен знаменитый парадокс Монти Холла, который легко решает с помощью формулы Байеса.

\subsection{Интеллектуальный анализ данных как объединение подходов}

\epigraph{В грамм добыча, в год труды.\\
	Изводишь единого слова ради\\
	Тысячи тонн словесной руды.}{В.~В.~Маяковский}

%Начало. Технологии, составляющие ДМ
Дальнейшее развитие статистических методов, особенно в их байесовском варианте, привело к возникновению следующего поколения методов статистического анализа, а именно методов машинного обучения. Первоначально эти методы развивались в двух направлениях, первое из которых представлено искусственными нейронными сетями, а второе --- деревьями принятия решений \cite[стр. 11-12]{handbook_stat_dm}.

Развитие методов машинного обучения в свою очередь привело к созданию статистической теории обучения (Statistical Learning Theory), которая направлена на решения проблемы предсказания на основе имеющихся данных \cite[стр. 12-13]{handbook_stat_dm}.
%Конец. Технологии, составляющие ДМ

%Начало. Место ДМ в структуре АД. Определение ДМ
Вышеуказанные сферы знания, пересекаясь друг с другом образуют новую дисциплину -- интеллектуальный анализ данных или data mining. Data mining --- это междисциплинарная область знания, находящая на пересечении традиционного статистического анализа, искусственного интеллекта, машинного обучения и развития больших баз данных \cite[стр. 5]{handbook_stat_dm}. Можно даже сказать, что data mining --- это новая философия, новый взгляд на анализ данных. 

%Начало: История ДМ
Хотя как самостоятельная дисциплина data mining окончательно оформился в 1990-х гг. \cite[стр. 15]{handbook_stat_dm}, о важности ухода от чистой математической статистики в пользу анализа реальных данных говорил ещё Джон Тъюки, который в 1962 году написал статью под названием <<Будущее анализа данных>> (The future of data analysis), в которой изложил основные идеи новой тенденции. Тьюки говорил о том, что излишняя сосредоточенность на математических теориях в статистики не помогает в решении реальных жизненных проблем. Он был убеждён, что анализ данных --- это работа, схожая с работой следователя и что надо дать данным говорить самим за себя. Однако эти идеи тогда не были восприняты приверженцами чистой математической статистики, которые утверждали, что правильная процедура статистического анализа прежде всего предполагает выдвижение научных гипотез, а затем уже  -- их проверку на основе полученных данных. Попытка анализа данных до выдвижения гипотезы категорически отвергалась, поскольку считалось, что это приведёт к смещению гипотезы в сторону того, что показали данные. Такая позиция привела к тому, что термин <<data mining>> стали использовать в уничижительном значении \cite[стр. 788]{HandbookCS}.

Развитие информационных технологий и вычислительной техники с одной стороны привело к появлению огромного количества данных, а с другой --- предоставило инструменты для их удобного сбора, хранения и обработки. Эти процессы также изменили течение академических споров, поскольку учёные осознали перспективы новой парадигмы анализа данных. Почему же data mining стал популярен в сложившихся условиях?

Суть философии data mining частично выражена в названии этой области знания, которое состоит из двух понятий: поиск ценной информации в большой базе данных (data) и добыча горной руды (mining). Именно в просеивании через сито своих инструментов огромного количества <<сырых>>, часто неструктурированных данных в поисках самородков, т. е. осмысленной, нетривиальной информации --- знаний. Более верным названием для этого процесса было бы <<knowledge mining from data>> (добыча знаний из данных) \cite[стр. 5]{Han2006}. Как видно, строки Маяковского, вынесенные в эпиграф, как нельзя лучше характеризуют интеллектуальный анализ данных.

Исходное определение термина, которое дал наш бывший соотечественник Григорий Пятнецкий-Шапито, звучит следующим образом: <<Data mining --- это процесс обнаружения в сырых данных ранее неизвестных нетривиальных практически полезных и доступных интерпретации знаний, необходимых для принятия решений в различных сферах человеческой деятельности>> \cite[стр. 78]{Duk2011}.

В статистике data mining часто иногда отождествляют с таким процессом как Knowledge Discovery in Databases, в то время как компьютерщики (computer scientists) предпочитают рассматривать первое определённую как часть второго.

Специфической областью data mining, нацеленной на анализ текстовых данных является text mining -- интеллектуальный анализ текста.

%Приведём ещё несколько определений DM.

%Как видно из вышеприведённых определений

\section{Методология интеллектуального анализа текста} \label{sect1_3}

По аналогии с термином data mining термину text mining можно дать следующее определение -- это нетривиальный процесс обнаружения действительно новых, потенциально полезных и понятных шаблонов в неструктурированных текстовых данных \cite[стр. 211]{bargesyan2009}.

Главная цель text mining состоит в обработке неструктурированного текста и, если это требует решаемая с помощью данного метода проблема, слабоструктурированных и структурированных данных, с тем, чтобы извлечь новое, значимое и применимое знание для лучшего принятия решений\cite[стр. 78]{practical_tm}.

Так как по сравнению с остальными устоявшимися статистическими методами text mining является относительно новой и неустоявшейся областью знания, сложно говорить, о наличии единой и общепринятой совокупности методов, направленных на получение устойчивого результата, т. е. о методологии. Во многом, исследователи, использующие методы text mining, руководствуются собственным опытом, приобретённым методом проб и ошибок, и создают собственную методологию. Наиболее значимые причины такого волюнтаризма включают следующее \cite[стр. 74]{practical_tm}:

\begin{itemize}
\item Разные исследователи вкладывают в понятие text mining разные значения. Данное определение ещё только формируется.
\item Неструктурированный характер данных открывает широкие возможности для действий исследователя.
%\item Существуют несколько форматов неструктурированных данных, некоторые из которых могут быть классифицированы как полуструктурированные (HTML, XML, JSON и другие).
%\item Огромные объёмы данных часто требуют сокращения и упрощения.
\end{itemize}

Самым популярным вариантом методологии data mining является CRISP-DM (CRoss Industry Standard Process for Data Mining) -- Межотраслевой стандартный процесс для data mining. Так как главное отличие text mining от data mining заключается в том, что первый специализируется на определённом типе данных, с небольшими изменениями CRISP-DM можно применить и для анализа текстовых данных. Весь цикл обработки данных это методологии представлен шестью последовательными этапами \cite[стр. 74]{practical_tm}.

\paragraph{Этап 1. Определение целей исследования.} С этого начинается практически любая осмысленная деятельность. Грамотная постановка цели требует глубокого понимания всех аспектов ситуации, в которой проводится исследование, и чёткого определения результата, который мы хотим получить. Для этого необходимо изучить проблему, на решение которой направлено исследование.

\paragraph{Этап 2. Оценка доступности и характера данных.} Данный этап включает в себя следующие задачи: 
	\begin{itemize}
	\item Определение источников текста. Текст может иметь цифровую форму или быть написан на бумаге, может находится внутри или за пределами исследуемой организации.
	\item Оценка доступности и применимости данных.
	\item Сбор первичных данных.
	\item Оценка содержательности данных (содержится ли в них необходимая для исследования информация).
	\item Оценка количества и качества данных. 
	\end{itemize}

После того, как разведывательная часть исследования успешно завершена, можно приступить к сбору данных из различных источников.

\paragraph{Этап 3. Подготовка данных.} Подготовка данных -- необходимый для text mining этап, ведь специфика данного метода по сравнению с data mining заключается в более трудоёмких стадиях сбора и обработки данных\cite[стр. 77]{practical_tm}. Это следствие неструктурированности или слабой структурированности данных. Этап подготовки данных состоит из следующих фаз:

	\subparagraph{Создание корпуса.} В лингвистике корпус -- это большой структурированный набор текстов. На данном этапе необходимо собрать все текстовые документы, относящиеся к исследуемой проблеме. Исследователю предстоит решить, какие данные и в каких объёмах необходимо собрать и проанализировать, чтобы решить поставленную задачу. Следует помнить, что все методы data mining сильно зависимы от точности полученных результатов от их количества.
	
	После того, как документы будут собраны, их необходимо трансформировать таким образом, чтобы они были представлены в единой форме (например, в базе данных или текстовом файле) для компьютерной обработки.

	\subparagraph{Предварительная обработка данных.} На данном этапе мы должны решить одну из главных проблем анализа текстов, а именно большое количество лишних слов в документе \cite[стр. 213]{bargesyan2009}, которые только создают помехи при включении их в анализ. Таким образом целью данного этапа будет удаление несущественных и вносящих помехи данных и преобразование данных к удобному для анализа виду. При подготовке данных обычно используют следующие приёмы:
	
	\begin{itemize}
	
	\item Удаление стоп-слов. Стоп-словами называются слова, которые являются вспомогательными и несут мало информации о содержании документа. Обычно заранее составляются списки таких слов, и в процессе предварительной обработки они удаляются из текста. Типичным примером таких слов являются вспомогательные слова и артикли, например: <<так как>>, <<кроме того>> и т. п.
	
	\item Cтемминг или лемматизация терминов, т. е. приведение их к простейшей форме, чаще всего к корню или к начальной форме слова (1-е лицо, единственное число, именительный падеж). Например, слова <<социолог>>, <<социологический>> и <<социология>> различны, но относятся к одной и той же теме. Вследствие процедуры стемминга, основанного на приведении к корню, всё они будут приведены к одному термину <<социолог>>. Это позволит сократить количество терминов и увеличить их частоту.
	
	\item Вышеописанные приёмы значительно уменьшают количество терминов в корпусе. Обычно после этого на основе корпуса обычно создаётся матрица терминов (document-term matrix), строками которой являются отдельные документы корпуса, а колонками -- уникальные термины. Соответственно в ячейках матрицы записывается число повторений терминов в документах. Представленные в таком виде текстовые данные удобно использовать для дальнейшего анализа.

	\end{itemize}

	
\paragraph{Этап 4. Разработка и калибровка модели.} На этом этапе происходит применение методов извлечения знаний. В text mining используется четыре основных метода: классификация, кластеризация, ассоциация, анализ трендов.
	
	\subparagraph{Классификация.} Вероятно, наиболее распространённым методом, использующимся в интеллектуальном анализе данных, является распределение объектов по классам согласно каким-либо важным признакам. В отношении к text mining эта задача известна как \textit{категоризация текста} и заключается в нахождении верной темы или понятия для каждого документа из корпуса. Сегодня автоматическая категоризация текста применяется в контексте различных задач, включая фильтрацию от спама, определение жанра, категоризацию веб-страниц в иерархических каталогах и многое другое.
	
	Существует два основных подхода к классификации текста. В первом подходе знания экспертов о категориях кодируются в правила, на основе которых объект относится к тому или иному классу. Второй подход, пришедший из машинного обучения, построен на работе определённого алгоритма, который обучившись на уже классифицированном наборе данных, способен в дальнейшем с некоторой вероятностью определять класс остальных объектов.
	
	\subparagraph{Кластеризация.} Кластеризация -- это упорядочивающая объекты в сравнительно однородные группы. Задача кластеризации относится к классу задач обучения без учителя. Это означает, что в процессе кластеризации не используется какая-либо предварительная информация о характеристиках групп, которые должны получится в итоге. В этом отличие кластеризации от классификации, где для определения класса объекта используется обучающая выборка или знания экспертов (происходит обучение с учителем).
	%Написать, где используется кластеризация
	
	\subparagraph{Создание правил ассоциации.} Ассоциация -- это процесс поиска повторяющихся образцов в группе объектов. Этот метод используется в интернет магазинах, чтобы на основании выбранных пользователем товаров предложить ему другие	 варианты. Главная идея этого метода в том, чтобы определить, правила, на основании которых определённые и часто непохожие между собой объекты объединяются в единый набор.
	
	В text mining данный метод используется чтобы измерить отношения между понятиями или группами понятий ($X \Rightarrow Y$).

\paragraph{Этап 5. Проверка результатов.} После того, как модель создана и настроена, мы должны произвести общую проверку всех действий. Например, необходимо убедиться, что выборка произведена правильно. Также случается, что в процессе построения исследования теряется основная цель, для достижения которой оно начиналось. На данном этапе следует проверить, решает ли модель сформулированную проблему и служит ли, таким образом, достижению цели. Если что-то упущено, необходимо вернуться назад к этапу, породившему рассогласованность между целью и результатом.

\paragraph{Этап 6. Внедрение.} В случае, если по итогам проверок было решено, что модель решает поставленную проблему, её можно применять. В самом простом случае внедрение может принимать форму написания отчёта о результатах исследования. В сложном -- построение интеллектуальной системы на основе построенной модели с тем, чтобы она могла быть повторно использована для принятия решений.

\section{Область применения и примеры использования методов интеллектуального анализа текста} \label{sect1_4}

Интеллектуальный анализ текста находит своё применение во многих областях. В экономике с его помощью можно установить, как настроения в СМИ влияют на котировки фондового рынка\cite{Tetlock2007}, имеется ли связь между отзывами о продукте в Интернет-магазине и его продажами\cite{mining_consumer_reviews}, как макроэкономические показатели могут быть измерены поисковыми запросами\cite{Google_econometrics} и текстами из социальных медиа.

В психологии этот метод позволяет узнать, как психическое состояние человека выражается в его языке\cite{psychological_meaning} и правда ли, что суточные и сезонные циклы настроения носят надкультурный характер\cite{seasonal_mood}.

Одним из самых известных и ранних примеров применения методов text mining в исторических исследованиях является установление авторства сборника статей <<Федералист>>\cite{federalist}. Здесь text mining принял форму стилометрии. Другое исследование в области text mining продемонстрировало, что в XVIII понятием <<литература>> объединялся более широкий класс явлений, чем сегодня\cite{encyclopedie}. 

Социолингвисты использовали text mining для идентификации географически зависимых лингвистических переменных и, на основании этого, предсказания местоположения пользователя на основе написанного им текста\cite{geographic_lexical_variation}.

Text-mining также можно использовать в качестве вспомогательного метода, уточняющего результаты традиционных опросов\cite{tweets_to_polls}.

Рассматриваемый метод активно используется в политологических и социологических исследованиях. Так как в данной работе будет представлено исследование именно такого вида, рассмотрим его подробней.

В 2012 году была опубликована работа, посвящённая выявлению политических предпочтений бельгийских Интернет-СМИ в ситуации политического кризиса \cite{MediaCoverage2012}. Суть кризиса состояла в том, что на протяжении более чем полутора лет ведущие валлонские и фламандские партии не могли договориться о составе федерального правительства. Корпус документов, используемых в исследовании, составили 68 000 статей, опубликованные в онлайн версиях восьми крупнейших фламандских газет в период с начала 2011 года до завершения политического кризиса в октябре того же года. Помимо даты публикации, критерием выбора статьи для анализа служило наличие в ней ключевых слов. Такими ключевыми словами считались названия фламандских политических партий, имеющих, по крайней мере, одно место в парламенте, и имена их важнейших представителей.

Первичная обработка данных включала удаление дубликатов. Затем на основе тонального словаря из более чем 3000 прилагательных, которые чаще всего встречались в отзывах на товары и которые были вручную проранжированы по шкале полярности (1 -- позитивное, -1 -- негативное) и субъективности (0 -- объективное, 1 -- субъективное), в каждой статье был произведён анализ тональности упоминаний выбранных политических партиях и политиках. Для этого подсчитывалась полярность каждого прилагательного в пределах двух предложений до и двух предложений после упоминания партии. Для уменьшения шума исключались прилагательные, набравшие меньше 0,1 и больше -0,1 очка по шкале полярности. В результаты было выделено 360 613 оценок.

Следующий шаг в данном исследовании -- определение степени представленности и популярности политической партии. Степень представленности политического субъекта в газете определялась через отношение количества статей газеты, где упоминалась данная партия, к количеству всех статей данной газеты.

Популярность политического субъекта определялась через относительное количество голосов, отданных за неё в результате голосования в 2010 году.

Популярность использовалась в качестве априорного распределения для расчёта степени склонности газеты к освещению определённой политической партии. Данная склонность определялась как разность между представленностью партии в газете и её реальной популярностью, определённой в результате выборов.

Таким образом было выявлено, какие политические субъекты пользуются популярностью электронных СМИ в большей или меньшей степени, чем среди населения в целом.

Следующий шаг -- выявление тональности упоминания политических партий и их представителей. Для каждого субъекта было подсчитано количество положительных и отрицательных отзывов, составлен график изменения тональности во времени.

В результате исследования при помощи методов анализа текстов были выявлены политические предпочтения главных фламандских новостных сайтов во время политического кризиса.

Другие методы были применены для выявления различий в освещении событий, приведших к восстанию 2011 года в Египте, египетскими государственными и негосударственными СМИ \cite{EgyptianUprising2012}. Материал для анализа составили более 29 000 новостных статей, вышедших в 2010--2011 годах. В методологической части работы был использован такой метод тематического моделирования как латентное размещение Дирихле (LDA), с помощью которого можно выполнить задачу категоризации документов (такой же метод будет использован в данной работе).

Было показано, что правительственные СМИ при освещении таких событий акцентировали внимание на угрозе дестабилизации и терроризма и старались рассказывать проведении реформ в стране. Независимые же СМИ наоборот были нацелены на мобилизацию в целях противостоянию режиму и фактически игнорировали действия правительства. Таким образом, было доказано, что режим Хосни Мумбарака потерял контроль на медиадискурсом ещё до начала активной фазы протестов.

Существуют примеры использование методов text-mining и в отечественных исследованиях. Дальше всего в этой сфере продвинулась сотрудники Лабораторией интернет-исследований Санкт-Петербургского филиала НИУ-ВШЭ. Исследовательский коллектив лаборатории в рамках проекта <<Разработка методологии сетевого и семантического анализа блогов для социологических задач>> поставил перед собой задачу выявления на больших массивах данных русскоязычной блогосферы тематические кластеры постов (о чем говорят?) и сообществ, основанные на комментировании (кто с кем говорит?), а также выяснения того, совпадают ли комментовые сообщества с тематическими кластерами (т.е. основана ли общность комментирования на общности темы?).

Эмпирический материал исследования составили 7941 статей топовых блогеров Живого Журнала за период 21-23 и 24-26 декабря 2011 года и комментарии к ним, собранные с помощью программы <<Blogminer>>. Выбор записей с таким временем написания был обусловлен тем, что именно в это время ожидалась реакция со стороны <<населения>> российской блогосферы на выборы в Государственную Думу, состоявшиеся 4 декабря.

%Для анализа данных использовалась программа NodeXL. Сообщества выявлялись путём применения алгоритмов Вакита-Цуруми и Клозэ-Ньюмана-Мура в качестве контрольного.

После операции по выявлению сообществ, которая разделила полную сеть постов на отдельные подмножества исследователи отобрали несколько групп постов для качественного анализа. Его целью было установить, связаны ли посты, входящие в одну группу по смыслу (тематически) или каким-либо другим образом (принадлежат перу одного или нескольких авторов).

По результатам качественного изучения постов из автоматически составленных групп был сделан вывод, что гипотеза исследования не подтвердилась: не были найдены доказательства того, что комментовые сообщества интегрированы общими темами в Живом Журнале.

Несмотря на неподтверждение гипотезы исследования, участие в проекте дало исследователям богатый опыт в организации Интернет-исследований, в результате чего была написана статья <<К методологии сбора Интернет-данных для социологического анализа>> \cite{methodlogy_internet}.
% Статья Кольцовой Применение автоматических методов анализа текстов для выявления тематической структуры Российской блогосферы ещё не доступна. Появится здесь http://www.isras.ru/4M_36.html

\section{Отличие интеллектуального анализа данных от контент-анализа}
% Постановка проблемы
% Время
% Различия в методе
% Различия в типе входных данных
% Использование компьютеров (в текст майнинг обязательно)

% Постановка проблемы: начало
После поверхностного взгляда на методы text mining может сложиться впечатление, что они повторяют уже хорошо известный социологом контент-анализ. И правда, из-за своего широкого распространения термин <<контент-анализ>> иногда используют как обобщающий для всех методов систематического и претендующего на объективность анализа политических текстов и текстов, циркулирующих в каналах массовой коммуникации. Однако такое расширительное понимание контент-анализа неправомерно, поскольку существует ряд исследовательских методов, которые не могут быть сведены к стандартному контент-анализу даже при максимально широком его понимании \cite{Iudin2010}. Обладая большим количеством общих черт, эти методы тем не менее имеют существенные отличия, что оправдывает их выделение в отдельные группы.

Что интересно, хотя методы контент-анализа и text mining имеют много общего, исследователи, работающие в каждой из этих двух сфер, редко ссылаются друг на друга. В литературе о text mining почти никогда не упоминаются методы контент-анализа и наоборот. Ещё сложнее найти источники, где сравниваются два данных метода. Как нам видится, причина такой ситуации прежде всего кроется 1) в недостаточной осведомлённости исследователей о методах интеллектуального анализа текста, 2) отсутствии однозначного определения как контент-анализа \cite[стр. 156]{Lande2006}, так и (в ещё большей степени) интеллектуального анализа текста, 3) и, о чём уже говорилось выше, привычкой называть любой метод анализа текстов контент-анализом.

%Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding
%Социологи обычно анализируют тексты одним из трёх способов. Некоторые просто читают их и делают выводы, основываясь на своих собственных предположениях, соответствующих поставленной задачи. Такой подход никак нельзя назвать научным, поскольку его результаты не воспроизводимы. Вторая распространённая стратегия заключается в конструировании набора тем (основываясь на каких-либо теоретических предпосылках или внимательном прочтении нескольких текстов), создании кодировочной инструкции и, затем, ручном кодировании текстов. Однако данная стратегия применима лишь для ограниченного количества текстов, поскольку даже команде кодировщиков не справиться с задачей обработки сотни тысяч текстов за приемлемое время. К тому же в таком случае во весь рост встанет проблема интеркодирования -- сложно добиться одинакового прочтения текста различными кодировщиками. И наконец, данный подход подразумевает, что исследователь обладает какими-либо предварительными знаниями, чтобы сконструировать набор тем, что не всегда верно.

Итак, как соотносятся такие понятия как контент-анализ и text mining?

% Время: начало
Если рассматривать временной критерий, то контент-анализ возник раньше text miming. В советской социологической литературе происхождение контент-анализа связывалось с именами У. Томаса и Ф. Знанецкого. Сейчас же многие зарубежные\cite{Smith2000} и отечественные\cite{Pochepcov1998} исследователи отмечают, что он возник сто и более лет тому назад, упоминая опыт использования метода, очень близкого к этому, когда в XIII веке в Швеции был осуществлён анализ сборника из 90 церковных гимнов, прошедших государственную цензуру и приобретших большую популярность, но обвинённых в несоответствии религиозным догматам. Наличие или отсутствие такого соответствия и определялось подсчётом в текстах этих гимнов религиозных символов и сравнения их с другими религиозными текстами, в том числе тех, которые считались еретическими. Частота использования определённых заранее собранных слов и тем позволяла судить о том, насколько корректен текст с точки зрения официального учения церкви. Как сформировавшийся метод анализа контент-анализ впервые был использован Максом Вебером в 1910 году для анализа освещаемости прессой политических акций в Германии \cite[стр. 155]{Lande2006}.

История методов text mining насчитывает гораздо меньше времени, поскольку тесно связана с развитием вычислительной техники и сопутствующих ей дисциплин, таких как искусственный интеллект, обработка естественного языка. Развитие информационных технологий привело к взрывообразному росту количества информации. Этот рост иллюстрирует тот факт, что количество веб-страниц в Интернете возросло с 10 миллионов в 2001 г. до 150 миллиардов в 2009 \cite[стр. 4]{practical_tm}. Для описания нового характера данных, которые отличаются большим объёмом, высокой скоростью роста и значительным многообразием свои форм, в специальном номере журнала <<Nature>> от 3 сентября 2008 года был введён термин <<большие данные>> (big data). Феномен <<больших данных>> создал потребность в новых методах обработки и анализа, способных извлекать полезное знание из ранее невиданных объёмов неструктурированной текстовой информации. Можно сказать, что методы text mining предназначены в первую очередь для анализа <<больших данных>>.

%Появление таких данных поставило проблему доступности необходимой информации. Эта проблема решалась двумя способами: с помощью информационного поиска (information retrieval) и извлечения информации (information extraction). Суть первого процесса состояла в выявлении в некотором множестве документов всех тех, которые удовлетворяют поисковому запросу. Второй процесс заключался в извлечении необходимых данных из набора документов \cite[стр. 5]{practical_tm}.

%Стоит ли вообще об этом говорить
%Решение этой проблемы заключалось в выполнении двух основных задач. Вначале надо произвести суммаризацию текстов (создание для каждого текста краткого содержания), которая бы уменьшила их размер и, таким образом, облегчила обработку. Затем необходимо классифицировать полученные содержания по некоторым категориям \cite[стр. 5]{practical_tm}.

%Подходы к доступу к текстовой информации развивались по влиянием трёх дисциплин: библиотековедения, информационной науки (information science) и обработке естественного языка \cite[стр. 5]{practical_tm}.

%Одним из самых ранних примеров классификации текстов был библиотечный каталог, который представлял собой совокупность расположенных по определенным правилам библиографических записей на документы. Следующим шагом была суммаризация текста для составления аннотаций. В 1958 году Питер Лун (Peter Luhn) приспособил первый коммерческий компьютер для научных вычислений IBM 701 для автоматического составления аннотаций. Для этого с помощью частотного анализа он рассчитал относительную значимость слов, а затем отобрал предложения, в которых было больше всего значимых слов с наименьшим расстоянием между ними. С небольшими изменениями эта методика использовалась на протяжении десятков лет.

%Создание в 1948 году Клодом Шенноном теории информации привело к возникновению информационной науки и дальнейшему развитию анализа текстов. Положения этой теории использовались за создания индекса цитирования научных статей, указывающего на значимость статьи и вычисляющегося на основе последующих публикаций, ссылающихся на данную работу. Дальнейшее использование теории информации применительно к анализу текстов повлияло на развитие поисковых систем.

%Соединение информационной науки с лингвистикой породило такую гибридную дисциплину, как обработка естественного языка. Ещё задача состояла в понимании и моделировании естественного языка и, таким образом, разработки систем компьютерного перевода текста. С этой дисциплиной частично пересекается компьютерная лингвистика. Являясь ответвлением от науки искусственного интеллекта, она ставит своей целью использование математических моделей для описания естественных языков. Именно в дисциплине обработки естественного языка были разработаны такие широко используемые сейчас приёмы как токенизация и стемминиг и метод кластеризации документов.
% Время: конец добавить по КА

% Методология: начало

Другое различие между контент-анализом и интеллектуальным анализом текста заключается различии алгоритмов их реализации. Из <<Рабочей книги социолога>>\cite{soc_workbook} мы знаем, что контент-анализ относится к формализованным методам анализа документов, суть которых <<сводится к тому, чтобы найти такие легко подсчитываемые признаки, черты, свойства документа (например, частота употребления определённых терминов), которые с необходимостью отражали бы определённые существенные стороны содержания с тем, чтобы сделать его доступным точным вычислительным операциям. В настоящее время, программы, ориентированные на контент-анализ используют основанные на классической статистике алгоритмы \cite[стр. 735]{tm_and_ca}. Контент-анализ не занимается собственно смыслом, а исключительно частотным распределением смысловых единиц в тексте, или по другому - анализом статических закономерностей частотного распределения смысловых единиц в тексте, и не более того \cite[стр. 15]{Averianov2007}. 

В процедуре контент-анализа можно выделить несколько этапов \cite[стр. 12-13]{Morozova2007}. Основа контент-анализа – это подсчёт встречаемости некоторых компонентов в анализируемом информационном массиве, дополняемый выявлением статистических взаимосвязей и анализом структурных связей между ними, а также снабжением их теми или иными количественными или качественными характеристиками. Таким образом, на первом этапе контент-анализа необходимо выбрать то, что необходимо считать, т. е. определить единицы текста. Этими единицами могут быть слова, абзацы, статьи или квадратные сантиметры площади, которую занимает текст. Следующий этап -- составление кодировочной инструкции и кодирование, т. е. трансформация и агрегация исходных данных в категории, которые позволяют точно описать характеристики текста, релевантные для исследования. На этом этапе исследователь подсчитывает количество появлений слова в тексте или решает, относится ли текст к определённой категории (например, определяет наличие в его содержании эротики). Контент-анализ заканчивается статистической обработкой полученных количественных данных (обычно используются процентные и частотные распределения, разнообразные коэффициенты корреляций) и их интерпретацией.

С другой стороны, для text mining используются методы анализа, основанные главным образом на байесовском подходе к статистике. Цель интеллектуального анализа текста состоит не в подсчёте частоты некоторых выделенных единиц, а в получении нового, ранее неизвестного знания. Его результатом скорее всего будет сложная математическая модель, распределяющая документы по заданным категориям или объединяющая их в кластеры.
% Методология: конец

Рассматриваемые методы различаются также в источниках входных данных \cite[стр. 735]{tm_and_ca}. Исторически контент-анализ применялся главным образом для анализа социологических, политологических или психологических данных, в то время как с помощью text-mining сейчас анализируют любые текстовые данные. Однако что касается типа данных, то контент-анализ является более универсальным методом, поскольку может быть применим для анализа документов самого разнообразного типа -- это могут быть визуальные изображения, устная речь, невербальное поведение и т. д. Text mining же по определению является сферой data mining, ограниченной анализом текстовых данных.

Между этими методами существует ещё одно различие, которое заключается в интенсивности использования компьютеров. Контент-анализ возник ещё задолго до изобретения первых ЭВМ и до сих пор предполагает ограниченное использования компьютера: если подсчёт слов легко можно автоматизировать, то процедура кодирования требует непосредственного участия исследователя. Text mining же с самого начала развивался вместе с развитием электронно-вычислительной техники. К тому же его связь с наукой искусственного интеллекта отражается в том, что после программирования модели вмешательство человека часто почти не требуется. Впрочем, это различие постепенно сходит на нет, поскольку сейчас появляются компьютерные системы автоматического контент-анализа, а для успешного интеллектуального анализа текста необходимо пристальное внимание исследователя на всех его этапах.

Однако, следует заметить, что изложенная нами позиция по определению терминов контент-анализа и text mining не единственная. Как говорилось ранее, существует большая путаница по разграничению объёмов этих понятий. Тем не менее, на наш взгляд, необходимо понимать, что данные методы существенно отличаются друг от друга, и главное отличие заключается в заложенных в них алгоритмах -- статистический подсчёт выделенных признаков с одной стороны и создание математической модели с другой.
%Некоторые отечественные исследователи вообще не оставляют места методам text mining в классификации методов анализа текста (http://pug.hse.ru/2012/02/20/n980). \todo[inline]{Сомнительная ссылка}
\clearpage