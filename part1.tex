\chapter{Text mining как метод анализа данных} \label{chapt1}
\section{Место text mining в структуре исследовательских методов} \label{sect1_1}
\epigraph{Если данные говорят с вами, значит вы --- байесовец.}{Филип А. Шродт \cite[стр. 11]{Schrodt2010}}
\epigraph{Bayes’ theorem is nominally a mathematical formula. But it is really much more than that. It implies that we must think differently about our ideas.}{Нэт Сильвер\footnotemark. The Signal and the Noise.}
\epigraph{В грамм добыча, в годы труды.\\
Изводишь единого слова ради\\
Тысячи тонн словесной руды.}{В.~В.~Маяковский}
\footnotetext{Американский статистик, давший самые точные прогнозы президентских выборов в США в 2008 и 2012 гг. Входит в 100 самых влиятельных людей в мире по версии журнала Times.}
\subsection{Дуальность статистики}
Дуальность статистики берёт своё начало из философского спора Аристотеля и Платона \cite[стр. 7]{handbook_stat_dm}. Аристотель считал, что реальность может быть познана только эмпирически и что исследователь должен тщательно изучать вещественный мир вокруг себя. Он пришёл к убеждению, что можно разложить сложную систему на элементы, детально описать эти элементы, соединить их вместе и, затем, понять целое. Именно таким механистичным путём долгое время следовала наука. Однако в дальнейшем стало понятно, что не всегда целое можно представить как простую сумму частей, его составляющих. Часто, будучи соединёнными вместе, совокупность этих частей приобретает новое качество.

В отличии от своего ученика, Платон считал что свойством подлинного бытия обладают только идеи, а человек может лишь воспринимать и воплощать в вещах их смутные очертания. Для Платона идея (целое) была большим, чем сумма её материальных проявлений.

Эта дихотомия восприятия реальности проявляется во многих аспектах человеческой мысли, в том числе и в сфере статистического знания, в котором с XVIII в. существует две основных философских позиции относительно того, как применять вероятностные модели. Первая определяет вероятность как нечто, заданное внешним миром. Вторая утверждает, что вероятность существует в головах людей.\cite[стр. 18]{Christensen2010}. В русле первого подхода возникли вначале классическая и затем развивающая её частотная концепции вероятности. Второй подход нашёл выражение в концепции байесовской вероятности.

Сторонники классического подхода исходят из того, что истинные параметры модели не случайны, а аппроксимирующие их оценки случайны, поскольку они являются функциями наблюдений, содержащих случайный элемент. \cite[стр. 5-6]{Zellner1980} Параметры модели считаются не случайными из-за того, что классическое определение вероятности исходит из предположения равновозможности как объективного свойства изучаемых явлений, основанного на их реальной симметрии \cite[стр. 24]{Gnedenko2005}. На такое представление о вероятности повлияло то, что в начале своего развития теория вероятности применялась прежде всего для анализа азартных игр. Суждение вида <<Вероятность выпадения шестёрки при бросании игрального кубика равняется 1/6>> основывается на том, что любая из шести граней при подбрасывании на удачу не имеет реальных преимуществ перед другими, и это не подлежит формальному определению. Таким образом, вероятностью случайного события $A$ в её классическом понимании будет называться отношение числа несовместимых (не могущих произойти одновременно) и равновозможных элементарных событий $m$ к числу всех возможных элементарных событий $n$:
\begin{eqnarray}
P(A)=\frac{m}{n}
\end{eqnarray}

Однако такое определение наталкивается на некоторые непреодолимые препятствия, связанные с тем, что не все явления подчиняются принципу симметрии. Например, из соображений симметрии невозможно определить вероятность наступления дождливой погоды. Для преодоления подобных трудностей был предложен статистический или частотный способ приближённой оценки неизвестной вероятности случайного события, основанный на длительном наблюдении над проявлением или не проявлением события $A$ при большом числе независимых испытаний и поиске устойчивых закономерностей числа проявлений этого события. Если в результате достаточно многочисленных наблюдений замечено, что частота события $A$ колеблется около некоторой постоянной, то мы скажем, что это событие имеет вероятность. Данный тип вероятности был выражен Р. Мизесом в следующей математической формуле:
\begin{eqnarray}
p=\lim_{x\to\infty}\frac{\mu}{n},
\end{eqnarray}
где $\mu$ --- количество успешных испытаний, $n$ --- количество всех испытаний \cite[стр. 46-47]{Gnedenko2005}. Вероятность здесь понимается как частота успешных исходов я является чисто объективной мерой, поскольку зависит лишь от точного подсчёта отношения количества успешных и неуспешных событий. 

Основываясь на этом подходе, статистика занималась созданием вероятностных моделей, которые включали в себя параметры, которые, как предполагалось, связаны с характеристиками исследуемой выборки. Параметры никогда не могут быть известны с абсолютной точностью до тех пор, пока мы не исследуем всю генеральную совокупность \cite[стр. 1]{Christensen2010}. До тех пор всегда существует вероятность отклонить гипотезу, когда она на самом деле верна, т. е. совершить ошибку первого рода. Для обозначения вероятности такой ошибки частотники используют понятие уровня значимости $\alpha$. Именно вероятность ошибки первого рода частотники ставят во главу анализа, определяя вероятность вероятность события. После каждого своего утверждения они обычно добавляют <<... на доверительном уровне в 95\%>>, подразумевая, что исследователь допускает вероятность ошибки в пяти процентах случаев (при $\alpha = 0,05$) \cite[стр. 10-11]{handbook_stat_dm}.

Иногда параметры вообще не возможно интерпретировать применительно к реальной жизни, поскольку модели редко бывают абсолютно верными. Модели, как мы надеемся, --- это некоторые полезные приближения к истине, на основании которых можно делать прогнозы. Тем не менее прежде всего классическое статистическое исследование  сосредоточено на оценке параметров, а не на предсказании \cite[стр. 1]{Christensen2010}.

%Развивая статистическую модель данных, байесовский подход предоставляет дополнительные вероятностные модели для всех неизвестных параметров в модели данных. Этот подход заключается в моделировании неуверенности в параметрах с помощью научной информации эксперта. Эта информация называется априорной. Экспертная информация должна быть получена независимо от анализируемых данных.
% Цель исследования: оценка параметров --- предсказание

Частотный подход доминировал в XX веке, придя на смену другому пониманию вероятности, связанном с именем английского математика Томаса Байеса\cite[стр. 2]{Efron2005}. Сущность байесовского подхода составляют три элемента: априорная вероятность, исходные статистические данных данные, постаприорная вероятность.

Байесовская статистика начинает построение своей модели при помощи понятия априорной вероятности, с помощью которой описывается текущее состояние наших знаний, относительно параметров распределения \cite[стр. 18]{Christensen2010}. Априорная вероятность, такие образом, --- это степень нашей уверенности в том, что исследуемый параметр примет то или иное значение ещё до начала сбора исходных статистических данных. На этом основании байесовское понимание вероятности относят к группе субъективистских трактовок вероятности. Чаще всего предполагается, что для оценки степени уверенности необходимо привлечь экспертов, чьё субъективное свидетельство позволит избежать действительной многократной реализации интересующего нас эксперимента\footnote{Не следует путать субъективный характер байесовской вероятности в целом с внутренним разделением сторонников данного подхода на объективистов и субъективистов, основанном на различном отношении к роли рациональных ограничений при определении априорной вероятности. В качестве примера различного подхода к определению априорной вероятности рассмотрим ситуацию, где событием является изъятие мячика из урны, наполненной красными и чёрными мячиками --- и это всё, что нам известно об урне. Зададим вопрос: какова априорная вероятность (до изъятия мячика), что изъятый мячик будет чёрного цвета? Субъективисты, считающие роль рациональных ограничений относительно небольшой, ответят, что любая вероятность от 0 до 1 может быть рациональной, так как по их мнению наша оценка априорной вероятности зависит большей частью от нерациональных факторов --- социализации, свободного выбора и др. Объективисты же будут настаивать, что априорная вероятность в данном случае равняется 1/2, поскольку именно такая вероятность в соответствии с принципом неопределённости Джейнса инвариантна к к размерам и трансформациям мячиков \cite{Talbott2013}.} \cite[стр. 34]{Aivazyan2001}.

Следующий элемент --- это исходные статистические данные. По мере их поступления статистик пересчитывает распределение вероятностей анализируемого параметра, переходя от априорного распределения к апостериорному, используя для этого формулу Байеса:
\begin{eqnarray}
P(A|B)=\frac{P(B|A)P(A)}{P(B)},
\end{eqnarray} где $P(A)$ --- априорная вероятность гипотезы $A$, $P(A|B)$ --- вероятность гипотезы $A$ при наступлении события $B$ (апостериорная вероятность), $P(B|A)$ --- вероятность наступления события $B$ при истинности гипотезы $A$, $P(B)$ --- полная вероятность наступления события $B$. Суть формулы в том, что она позволяет переставить причину и следствие: по известному факту события вычислить вероятность того, что оно было вызвано данной причиной. Эту формулу также называют формулой обратной вероятности. Процесс пересмотра вероятностей, связанных с высказываниями, по мере поступления новой информации составляет существо \textbf{обучения на опыте}\footnote{Понятие <<обучение на опыте>> ещё не раз встретится в данной работе, поскольку именно оно составляет суть машинного обучения --- подраздела науки искусственного интеллекта, методы которого используются в text-mining.} \cite[стр. 21-22]{Zellner1980} и  является одним из возможных способов формализации и операционализации следующего тезиса: <<\textit{степень нашей разумной уверенности в некотором утверждении (касающемся, например, неизвестного численного значения интересующего нас параметра) возрастает и корректируется по мере пополнения имеющейся у нас информации относительно исследуемого явления}>> \cite[стр. 93]{Aivazyan2008}. В частотном подходе данный тезис интерпретируется в свойстве состоятельности оценки неизвестного параметра: чем больше объём выборки, на основании которой мы строим свою оценку, тем большей информацией об этом параметре мы располагаем и тем ближе к истине наше заключение. Специфика байесовского подхода к интерпретации этого тезиса основана на том, что вероятность, понимаемая как количественное значение степени разумной уверенности в справедливости некоторого утверждения, пересматривается по мере изменения информации, касающейся этого утверждения. Поэтому в данном подходе вероятность всегда есть условная вероятность, при условии нынешнего состояния информации (в русле классического подхода исследователь скорее склонен рассматривать совместную вероятность\cite[стр. 5]{handbook_stat_dm}).

Дискуссии вокруг того, какой же метод предпочтительней, ведутся уже не одно столетие, породив великое множество книг и статей на эту тему \cite{Jeffreys1983}, \cite{Efron2005}, но к однозначному выводу прийти не удалось. Острота дискуссии объясняется тем, что спор сторонников байесовского и частотного подхода к статистическому выводу отражает два различных взгляда на способ добычи научного знания. Именно поэтому от ответа на этот, казалось бы, локальный вопрос математической статистики зависит развитие всей науки.

Так или иначе, в 1980-х годах, стало ясно, что частотный подход к статистическому выводу не достаточно хорошо подходит для анализа нелинейных отношений в больших объёмах данных, производимых сложными системами при моделировании процессов реального мира\cite[стр. 10]{handbook_stat_dm}. Для преодоления этих ограничений частотники создали нелинейные версии параметрических методов, такие как множественный нелинейный регрессионный анализ.

В то время как в частотном подходе происходили изменения, немногочисленные сторонники байесовского подхода упрямо продвигали свою точку зрения на модель статистического вывода. Как оказалось, байесовская модель лучше подходит для поиска ответов на некоторые практические вопросы, поскольку полнее учитывает прошлую информацию и располагает к предсказаниям. Например, намного важнее минимизировать вероятность ложноотрицательного диагностирования некоторой опухоли как раковой, чем вероятность её ложноположительного определения (ошибка первого рода). 


Продемонстрируем на примере различия в рабоче частотных и байесовских методов проверки гипотез. Предположим, некоторый стрелок утверждает, что точность его стрельбы составляет 75\%. Когда стрелка попросили продемонстрировать свои навыки, он попал в мишень только 2 раза из 8. Какова вероятность, что стрелок сказал правду о своих навыках.

\textbf{Решение задачи в частотном подходе}. Гипотеза $H_{0}$ --- стрелок сказал правду. Испытание --- стрельба по мишени. Событие $A$ --- попадание в мишень. $P(A)$ постоянная и равна 0,75. Для расчёта вероятности того, что событие $A$ наступило не более 2 раз в 8 независимых испытаниях, применим формулу Бернулли для количества успешных испытаний $k$ = 0, 1, 2 и получим, что $P(A\leq 2)=0,0042$. Следовательно, при уровне значимости $\alpha$ = 0,05 следует признать невероятным, что точность стрелка составляет 75\%, гипотеза $H_{0}$ отвергается. 

Отметим некоторые особенности данного решения. Во-первых, для решения задачи мы фактически использовали только умение рассчитывать совместную вероятность, ведь формула Бернулли является сокращённым видом расчёта совместной вероятности успешных комбинаций. Во-вторых, мы решили, что если гипотеза верна, то  вероятность отклонить гипотезу, когда она на самом деле верна должна быть не менее 5\%, т. е. нам важно, чтобы вероятность ложноположительного ответа была ниже определённой границы. Вероятность ложноотрацательного ответа не рассматривается.

\textbf{Решение задачи в байесовском подходе}. В данном подходе мы не проверяем гипотезу, а рассчитываем условную вероятность события $A$ (точность стрелка составляет 75\%) при условии события $B$ (стрелок попал в мишень не более 2 раз из 8). Прежде всего нам нужно оценить априорную вероятность события $A$. Это можно сделать посмотрев статистику стрельбы остальных стрелков. Предположим, мы выяснили, что 70\% стрелков имеют точность в 75\%. Следовательно, $P(A)=0,7$. $P(B|A)$ мы уже рассчитали в частотном подходе. $P(B)$ легко рассчитывается по формуле полной вероятности. По формуле Байеса $P(A|B)=0,0301$. 

Как видно из этого примера, в байесовском подходе другая логика рассчёт вероятности: на основании данных рассчитывается вероятность того, что $H_{0}$ верна, в то время как раньше мы рассчитывали вероятность того, что стрелок поразил мишень не более 2 раз в 8 независимых испытаниях. Данные, полученные с помощью данного метода, данные можно использовать более продуктивно. Предположим, что мы рассчитываем не вероятность того, что стрелок с определённым умениями поразил мишень какое-то количество раз, а вероятность наличия тяжёлого заболевания у человека с каким-то количеством положительных тестов. В случае частотного подхода мы узнаем, какова вероятность того, что больной человек получит н-ое поличество положительных тестов. Байесовский же подход позволяет узнть именно то, что нам надо --- вероятность того, что человек, получивший н-ое количество положительных тестов, болен. Другой плюс данных методов --- они работают даже если размер выборки равен нулю. В таком случае байесовская вероятность равна априорной.

Проведение тестирования на статистическую значимость оценивает лишь вероятность получения похожего результата с другим набором данных при сохранении тех же самых условий. Однако оно предоставляет ограниченную картину такой вероятности, поскольку в расчет принимается ограниченное количество информации относительно исследуемых данных. И оно само по себе не способно вам сказать, являются ли основные положения исследования верными и будут ли подтверждены полученные результаты в различных условиях \footnote{Роль статистической значимости в неудачах науки. URL: \url{http://inosmi.ru/world/20131114/214743342.html}}. Уровень p говорит только о вероятности получения результата при (обычно) совершенно нереалистичных условиях нулевой гипотезы. А это совсем не то, что мы хотим узнать, --- обычно мы хотим знать величину эффекта независимой переменной с учетом имеющихся данных. Это байесовск ий вопрос, а не частотный. Вместо этого значение p часто интерпретируется так, будто бы оно показывало силу ассоциации \cite[стр. 11]{Schrodt2010}.

С другой стороны у и байесовского метода имеются несколько недостатков. Одним из них является необходимость привлекать для рассчёта априорные данные, которые могут быть недоступны. А если они и доступны, то, как отмечалось выше, часто носят субъективный характер. Другой недостаток --- сложность вычислений. В вышеописанном примере для вычисления байесовской вероятности нам необходимо было вычислить частотную вероятность, полную вероятность, и, наконец, собственно байесовскую вероятность. Сложность байесовских вычислений частитчно объясняет тот факт, что байесовские методы вновь обрели популярность с развитием вычислительной техники. Следующий недостаток байесовского метода --- неинтуитивность, непонятность его результатов для обыденного сознания. Именно на этой неинтуитивности построен знаменитый парадокс Монти Холла, который легко решает с помощью формулы байеса.

\subsection{Data mining как объединение подходов}

%Начало. Технологии, составляющие ДМ
Дальнейшее развитие статистических методов, особенно в их байесовском варианите, привело к возникновению следующего поколения методов статистического анализа, а именно методов машинного обучения. Первоначально эти методы развивались в двух направлениях, первое из которых представлено искусственными нейронными сетями, а второе --- деревьями принятия решений \cite[стр. 11-12]{handbook_stat_dm}.

Развитие методов машинного обучения в свою очередь привело к созданию статистической теории обучения (Statistical Learning Theory), которая направлена на решения проблемы предсказания на основе имеющихся данных \cite[стр. 12-13]{handbook_stat_dm}.
%Конец. Технологии, составляющие ДМ

%Начало. Место ДМ в структуре АД. Определение ДМ
Какое место занимают метды Data Mining в описанной структуре? DM --- это междисциплинарная область знания, находящая на пересечении традиционного статистического анализа, искуственного интеллекта, машинного обучения и развития больших баз данных \cite[стр. 5]{handbook_stat_dm}. Можно даже сказать, что DM --- это новая философия, новый взгляд на анализ данных. 

%Начало: История ДМ
Хотя как самостоятельная дисциалина DM окончательно оформился в 1990-х гг. \cite[стр. 15]{handbook_stat_dm}, о важности ухода от чистой математической статистики в пользу анализа реальных данных говорил ещё Джон Тъюки, который в 1962 году написал статью под названием <<Будущее анализа данных>> (The future of data analysis), в которой изложил основные идеи новой тенденции. Тьюки говорил о том, что излишняя сосредоточенность на математических теориях в статистики не помогает в решении реальных жизненных проблем. Он был убеждён, что анализ данных --- это работа, схожая с работой следователя и что надо дать данным говорить самим за себя. Однако эти идеи тогда не были восприняты приверженцами чистой математической статистики, которые утверждали, что правильная процедура статистического анализа прежде всего предполагает выдвижение научных гипотез, а затем уже из проверку, на основе полученных данных. Попытка анализа данных до выдвижения гипотезы категорически отвергалась, поскольку считалось, что это приведёт к смещению гипотезы в сторону того, что показали данные. Такая позиция привела к тому, что термин <<DM>> стали использовать в уничижительном значении \cite[стр. 788]{HandbookCS}.

Развитие информационных технологий и вычислительной техники с одной стороны привело к появлению огромного количества данных, а с другой --- предоставило инструменты для их удобного сбора, хранения и обработки. Эти процессы также изменили течение академических споров, поскольку учёные осознали перспективы новой парадигмы анализа данных. Почему же DM стал популярен в сложившихся условиях?

Суть философии DM частично выражена в названии этой области знания, которое состоит из двух понятий: поиск ценной информации в большой базе данных (data) и добыча горной руды (mining). Именно в просеивании через сито своих инстументов огромного количества <<сырых>>, часто нестуктурированных данных в поисках самородков, т. е. осмысленной, нетривиальной информации --- знаний. Более верным названием для этого процесса было бы <<knowledge mining from data>> (добыча знаний из данных) \cite[стр. 5]{Han2006}.

Исходное определение термина, которое дал наш бывший соотечественник Григорий Пятнецкий-Шапито, звучит следующим образом: <<Data mining --- это процесс обнаружения в сырых данных ранее неизвестных нетривиальных практически полезных и доступных интерпретации знаний, необходимых для принятия решений в различных сферах человеческой деятельности>> \cite[стр. 78]{Duk2011}.

В статистике Data Mining часто иногда отождествяют с таким процессом как Knowledge Discovery in Databases, в то время как компьютерщики (computer scientists) предпочитают рассматривать первое определённую как часть второго.

%Приведём ещё несколько определений DM.

%Как видно из вышеприведённых определений

Главная цель text mining состоит в обработке неструктурированного текста и, если это требует решаемая с помощью данного метода проблема, слабоструктурированных и стуктурированных данных, с тем, чтобы извлечь новое, значимое и применимое знание для лучшего принятия решений\cite[стр. 78]{practical_tm}.

\section{Методология text mining} \label{sect1_3}
Так как по сравнению с остальными устоявшимися статистическими методами text mining является относительно новой и неустоявшейся областью знания, сложно говорить, о наличии единой и общепринятой совокупности методов, направленных на получение устойчивого результата, т. е. о методологии. Во многом, исследователи, использующие методы text mining, руководствуются собственным опытом, приобретённым методом проб и ошибок, и создают собственную методологию. Наиболее значимые причины такого волюнтаризма включают следующее:
\begin{itemize}
\item Само понятие text mining для разных людей может означать разные вещи. Данное определение ещё только формируется.
\item Неструктурированный характер данных открывает широкие возможности для действий исследователя.
\item Существует несколько форматов неструктурированных данных, некоторые из которых могут быть классифицированы как полуструктурированные (HTML, XML, JSON и другие).
\item Огромные объёмы данных часто требуют сокращения и упрощения.
\end{itemize}

Самым популярным вариантом методологии Data-Mining является CRISP-DM (CRoss Industry Standard Process for Data Mining) -- Стандартный межотраслевой процесс Data Mining. Так как главное отличие Text Mining от Data Mining заключается в том, что первый специализируется на определённом типе данных, с небольшими изменениями CRISP-DM можно применить и для него. Весь цикл обработки данных это методологии представлен шестью последовательными этапами.

\paragraph{Этап 1. Определение целей исследования.} С этого начинается практически любая осмысленная деятельность. Грамотная постановка цели требует глубокого понимания всех аспектов ситуации, в которой проводится исследование и чёткого определения результата, который мы хотим получить. Для этого необходимо изучить проблему, на решение которой направлено исследование.

\paragraph{Этап 2. Оценка доступности и характера данных.} Данный этап включает в себя следующие задачи: 
	\begin{itemize}
	\item Определение источников текста. Текст может иметь цифровую форму или написан на бумаге, находится внутри или за пределами организации.
	\item Оценка доступности и применимости данных.
	\item Сбор первичных данных.
	\item Оценка содержательности данных (содержится ли в них необходимая для исследования информация).
	\item Оценка количества и качества данных. 
	\end{itemize}
	После получения удовлетворительных результатов можно приступать к интеграции данных из различных источников.
	
\paragraph{Этап 3. Подготовка данных.} Подготовка данных -- необходимый для text mining этап. По мнению многих, специфика данного метода по сравнению с data mining заключается в более трудоёмких стадиях сбора и обработки данных\cite[стр. 77]{practical_tm}.
	\subparagraph{Создание корпуса.} В лингвистике корпус -- это большой структурированный набор текстов. На данном этапе необходимо собрать все текстовые документы, относящиеся к исследуемой проблеме. Особенность data mining заключается в сильной зависимости точности полученных результатов от их количества.
	
	После того, как документы будут собраны, их необходимо трансформировать таким образом, чтобы они были представлены в единой форме (например в базе данных SQL или текстовом файле) для компьютерной обработки.
	
	%Узнать, обязательно ли создавать матрицу
	\subparagraph{Предварительная обработка данных.} На этой стадии на основе корпуса создаётся матрица терминов (document-term matrix), строками которой являются отдельные документы корпуса, а колонками -- уникальные термины. Соответственно в ячейках матрицы записывается число повторений терминов в документах.
	
	Перед создание матрицы для удобства применения алгоритмов анализа необходимо уменьшить количество терминов в корпусе. Для выполнения этой задачи существует несколько приёмов. Первый -- удаление из матрицы стоп-слов, т. е. тех слов, которые нельзя содержательно интерпретировать в последующем анализе -- предлогов, союзов.
	
	Следующий шаг по упрощению -- это стёмминг или лемматизация терминов, т. е. приведение их к простейшей форме, чаще всего к корню. Например, слова "социолог", "социологический" и "социология" различны, но относятся к одной и той же теме. Вследствие процедуры стёмминга всё они будут приведены к одному термину "социолог". Это позволит сократить количество терминов и увеличить их частоту.
	
	После выполнения этих процедур можно приступить к созданию матрицы терминов.
	% Можно рассказать о конкретных алогоритмах создания матрицы.
	
\paragraph{Этап 4. Разработка и калибровка модели.} На этом этапе происходит применение методов извлечения знаний. В text mining используется четыре основных метода: классификация, кластеризация, ассоциация, анализ трендов.
	
	\subparagraph{Классификация.} Вероятно, наиболее распространённым методом, использующимся в интеллектуальном анализе данных является распределение объектов по классам согласно каким-либо важным признакам. В отношении к text mining эта задача известна как \textit{категоризация текста} и заключается в нахождении верной темы или понятия для каждого документа из корпуса. Сегодня автоматическая категоризация текста применяется в контекста различных задач, включая фильтрацию от спама, определение жанра, категориацию веб-страниц в иерархических каталогах и многое другое.
	
	Существует два основных подхода к классицикации текста. В первом подходе знания экспертов о категориях кодируются в правила, на основе которых объект относится к тому или иному классу. Второй подход, пришедший из машинного обучения, построен на работе определённого алгоритма, который обучившись на уже классифицированном наборе данных, способен в дальнейшем с некоторой вероятностью определять класс остальных  объектов.
	
	\subparagraph{Кластеризация.} Кластеризация -- это упорядочивающая объекты в сравнительно однородные группы. Задача кластеризации относится к классу задач обучения без учителя. Это означает, что в процессе кластеризации не используется какая-либо предварительная информация о характеристиках групп, которые должны получится в итоге. В этом отличие кластеризации от классификации, где для определения класса объекта используется обучающая выборка или знания экспертов (происходит обучение с учителем).
	%Написать, где используется кластеризация
	
	\subparagraph{Создание правил ассоциации.} Ассоциация -- это процесс поиска повторяющихся образцов в группе объектов. Этот метод используется в интернет магазинах, чтобы на основании выбранных пользователем товаров предложить ему другие	 варианты. Главная идея этого метода в том, чтобы определить, правила, на основании которых определённые и часто непохожие между собой объекты объединяются в единый набор.
	
	В text mining данный метод используется чтобы измерить отношения между понятиями или группами понятий. В правиле ассоциации $X \Rightarrow Y$

\paragraph{Этап 5. Проверка результатов.} После того, как модель создана и проверена, мы должны произвести общую проверку всех действий. Например, необходимо убедиться, что выборка произведена правильно. Также случается, что в процессе построения исследования теряется основная цель, для достижения которой оно начиналось. На данном этапе следует проверить, решает ли модель сформулированную проблему и служит ли, таким образом, достижению цели. Если что-то упущено, необходимо вернуться назад к этапу, породившему рассогласованность между целью и результатом.
\paragraph{Этап 6. Внедрение.} В случае, если по итогам проверок было решено, что модель решает поставленную проблему, её можно применять. В самом простом случае внедрение может принимать форму написания отчета о результатах исследования. В сложном -- построение интеллектуальной системы на основе построенной модели с тем, чтобы она могла быть повторно использована для принятия решений.

\section{Область применения и примеры использования методов text mining} \label{sect1_4}

Интеллектуальный анализ текста находит своё применение во многих областях. В экономике с его помощью можно установить, как настроения в СМИ влияют на котировки фондового рынка\cite{Tetlock2007}, имеется ли связь между отзывами о продукте в Интернет-магазине и его продажами\cite{mining_consumer_reviews}, как макроэкономические показатели могут быть измерены поисковыми запросами\cite{Google_econometrics} и текстами из социальных медиа.

В психологии этот метод позволяет узнать, как психическое состояние человека выражается в его языке\cite{psychological_meaning} и правда ли, что суточные и сезонные циклы настроения носят надкультурный характер\cite{seasonal_mood}.

Одним из самых известных и ранних примеров применения методов text mining в исторических исследованиях является установление авторства сборника статей <<Федералист>>\cite{federalist}. Здесь text mining принял форму стилометрии. Другое исследование в области text mining продемонстрировало, что в XVIII понятием <<литература>> объединялся более широкий класс явлений, чем сегодня\cite{encyclopedie}. 

Социолингвисты использовали text mining для идентификации географически зависимых лингвистических переменных и, на основании этого, предсказания местоположения пользователя на основе написанного им текста\cite{geographic_lexical_variation}.

Text-mining также можно использовать в качестве вспомогательного метода, уточняющего результаты традиционных опросов\cite{tweets_to_polls}.

Рассматриваемый метод активно используется в политологических и социологических исследованиях. Так как в данной работе будет представлено исследование именно такого вида, рассмотрим из подробней.

В 2012 году было опубликовано работа, посвящённая выявлению политических предпочтений бельгийских Интернет-СМИ в ситуации политического кризиса \cite{MediaCoverage2012}. Суть кризиса состояла в том, что на протяжении более чем полутора лет ведущие валлонские и фламандские партии не могли договориться о составе федерального правительства. Корпус документов, используемых в исследовании, составили 68 000 статей, опубликованные в онлайн версиях восьми крупнейших фламандских газет в период с начала 2011 года до завершения политического кризиса в октябре того же года. Помимо даты публикации, критерием выбора статьи для анализа служило наличие в ней ключевых слов. Такими ключевыми словами считались названия фламандских политических партий, имеющих покрайней мере одно место в парламенте, и имена их важнейших представителей.

Первичная обработка данных включала удаление дубликатов. Затем на основе тонального словаря из более чем 3000 прилагательных, которые чаще всего встречались в отзывах на товары и которые вручную были проранжированы по шкале полярности (1 -- позитивное, -1 -- негативное) и субъективности (0 -- объективное, 1 -- субъективное), в каждой статье был произведён анализ тональности упоминаний выбранных  политических партиях и политиках. Для этого подсчитывалась полярность каждого прилагательного в пределах двух предложений до и двух предложений после упоминания партии. Для уменьшения шума исключались прилагательные набравшие меньше 0,1 и больше -0,1 очка по шкале полярности. В результаты было выделено 360 613 оценок.

Следующий шаг в данном исследовании -- определение степени представленности и популярности политической партии. Степень представленности $coverage(e, s)$ политического субъекта $e$ в газете $s$ определялась как отношение количества статей газеты, где упоминалась данная партия, к количеству всех статей данной газеты $A_{s}$:
\begin{eqnarray}
coverage(e, s)=\frac{\# \{a|a \in A_{s} \wedge e \in a \}}{\# A_{s}}
\end{eqnarray}
Популярность $popularity(e)$ политического субъекта $e$ определялась через относительное количество голосов, отданных за неё в результате голосования в 2010 году $v(e)$:
\begin{eqnarray}
popularity(e)=\frac{v(e)}{\sum\limits_{e'\in\varepsilon}v(e')}
\end{eqnarray}
Популярность использовалась в качестве априорного распределения для рассчёта степени склонности газеты к освещению определённой политической партии. Данная сколнность определялась как разность между представленностью партии в газете и её реальной популярностю, определённой в резульате выборов:
\begin{eqnarray}
bias(e, s) = coverage(e, s) - popularity(e)
\end{eqnarray}
В результате данных манипуляций были выявлено, какие политические субъекты пользуются популярностью электронных СМИ в большей или меньшей степени, чем среди населения в целом.

Следующий шаг -- выявление тональности упоминания политических партий и их представителей. Для каждого субъекта было подсчитано количество положительных и отрицательных отзывов, составлен график изменения тональности во времени.

В резульате исследования при помощи методов анализа текстов были выявлены политические предпочтения главных фламандских новостных сайтов во время политического кризиса.

Более интеллектуальные методы были применены для выявления различий в освещении событий, приведших к восстанию 2011 года в Египте, египетскими государственными и негосударственными СМИ \cite{EgyptianUprising2012}. Материал для анализа составили более 29 000 новостных статей, вышедших в 2010--2011 годах. В методологической части работы был использован такой метод тематического моделирования как латентное размещение Дирихле (LDA), с помощью которого можно выполнить задачу категоризации документов. Алгоритм сам определяет оптимальное количество категорий (тем) и распределяет документы между ними. % можно разъяснить работу алгоритма

Было показано, что правительственные СМИ при освещении таких событий акцентировали внимание на угрозе дестабилизации и терроризма и старались рассказывать проведении реформ в стране. Независимые же СМИ наоборот были нацелены на мобилизацию в целях противостоянию режиму и фактически игнорировали действия правительства. Таким образом, было доказано, что режим Хосни Мумбарака потерял контроль на медиадискурсом ещё до начала активной фазы протестов.

% Ещё примеры применения метода

Существуют примеры использование методов text-mining и в отечественных исследованиях. Дальше всего в этой сфере продвинулась сотрудники НИУ-ВШЭ, в частности заведующая Лабораторией интернет-исследований Кольцова Елена Юрьевна. Исследовательский коллектив под её руководством в рамках проекта <<Разработка методологии сетевого и семантического анализа блогов для социологических задач>> поставил перед собой задачу выявления на больших массивах данных русскоязычной блогосферы тематические кластеры постов (о чем говорят?) и сообществ, основанные на комментировании (кто с кем говорит?), а также выяснения того, совпадают ли комментовые сообщества с тематическими кластерами (т.е. основана ли общность комментирования на общности темы?).

Тестовой тематикой являлась тема Ислама. Эмпирический материал исследования составили 7941 статей топовых блогеров Живого Журнала за период 21-23 и 24-26 декабря 2011 года и комментарии к ним, собранные с помощью паука краулера <<Blogminer>>. Выбор записей с таким временем написания был обусловлен тем, что именно в это время ожидалась реакция со стороны "населения" российской блогосферы на выборы в Государственную Думу, состоявшиеся 4 декабря.

Для анализа данных использовалась программа NodeXL. Сообщества выявлялись путём применения алгоритмов Вакита-Цуруми и Клозэ-Ньюмана-Мура в качестве контрольного.

После операции по выявлению сообществ, которая разделила полную сеть постов на отдельные подмножества исследователи отобрали несколько групп постов для качественного анализа. Его целью было установать, связаны ли посты, входящие в одну группу по смыслу (тематически) или каким-либо другим образом (принадлежат перу одного или нескольких авторов).

По резульатам качественного изучения постов из автоматически составленных групп был сделан вывод, что гипотеза исследования не подтвердилась: не были найдены доказательства того, что комментовые сообщества интегрированые общими темами в Живом Журнале.

Несмотря на неподтверждение гипотезы исследования, участие в проекте дало исследователям богатый опыт в организации Интернет-исследований, в результате чего была написана статья <<К методологии сбора Интернет-данных для социологического анализа>> \cite{methodlogy_internet}.
% Статья Кольцовой Применение автоматических методов анализа текстов для выявления тематической структуры Российской блогосферы ещё не доступна. Появится здесь http://www.isras.ru/4M_36.html

\clearpage